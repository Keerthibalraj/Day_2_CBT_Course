{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1fTxseRfOgfLNf3aX4Du_wSX34YVB5oI0",
      "authorship_tag": "ABX9TyMxZTHR55dxGuknvoQHR0yN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Keerthibalraj/Day_2_CBT_Course/blob/main/DAY_2_CBT_2023.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORT LIBRARY**"
      ],
      "metadata": {
        "id": "I3f9OJX_Y5oc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEOGkpWLBLcP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from sys import stdout\n",
        "import logging\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import scipy.io as sio\n",
        "from scipy.signal import savgol_filter\n",
        "import tqdm\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cross_decomposition import PLSRegression\n",
        "from sklearn.model_selection import cross_val_score , KFold\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.layers import Conv1D, Reshape\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, Callback,  ModelCheckpoint\n",
        "from tensorflow.keras.utils import plot_model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "wmxnqz3Bz0c2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "data = **missing code**\n",
        "print(data.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yHKAja1CEZ3",
        "outputId": "434c9105-75ea-47ce-967c-6e86b48045b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['__header__', '__version__', '__globals__', 'DM_cal', 'DM_test', 'Sp_cal', 'Sp_test', 'wave', 'SP_all_test', 'SP_all_train'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the dataset\n",
        "plt.figure(figsize=(10,6))\n",
        "\n",
        "plt.subplot(221)\n",
        "plt.title('Train data')\n",
        "plt.plot(data['wave'].T,data['Sp_cal'].T)\n",
        "plt.ylabel('Amplitude (a.u)')\n",
        "plt.xlabel('Wavelength (nm)')\n",
        "\n",
        "plt.subplot(222)\n",
        "plt.title('Test data')\n",
        "plt.plot(data['wave'].T,data['Sp_test'].T)\n",
        "plt.ylabel('Amplitude (a.u)')\n",
        "plt.xlabel('Wavelength (nm)')\n",
        "\n",
        "plt.subplot(223)\n",
        "plt.plot(data['DM_cal'])\n",
        "plt.ylabel('Dry Matter (%)')\n",
        "plt.xlabel('Sample number')\n",
        "\n",
        "plt.subplot(224)\n",
        "plt.plot(data['DM_test'])\n",
        "plt.ylabel('Dry Matter (%)')\n",
        "plt.xlabel('Sample number')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rpSISD7WEEAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Spectra (x) and target variable (Y)\n",
        "\n",
        "## Convert data type to float32 for better inter-operability with TensorFlow\n",
        "x_train=data['SP_all_train'].astype(np.float32)\n",
        "y_train=data['DM_cal'].astype(np.float32)\n",
        "x_test=data['SP_all_test'].astype(np.float32)\n",
        "y_test=data['DM_test'].astype(np.float32)\n"
      ],
      "metadata": {
        "id": "2TN3n5D1EXGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "61qH2utQzvOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Spliting the full train set into calib and tuning subsets. It is important to set the 'random_state'\n",
        "## parameter to a fixed value in order to guarentee that each time you run the experiment, the data is\n",
        "## split the same way\n",
        "X_train, X_val, Y_train, Y_val=** missing code**\n",
        "## The wavelenghts for the XX axis when we plot the spectra\n",
        "x_scale=data['wave'].astype(np.float32).reshape(-1,1)\n",
        "\n",
        "## Check for dimensions\n",
        "print('Data set dimensions ----------------------------')\n",
        "print('Full Train set dims X Y = {}\\t{}'.format(x_train.shape, y_train.shape))\n",
        "print('Training set dims X Y = {}\\t{}'.format(X_train.shape, Y_train.shape))\n",
        "print('Validation set dims X Y = {}\\t{}'.format(X_val.shape, Y_val.shape))\n",
        "print('Test set dims X Y = {}\\t{}'.format(x_test.shape, y_test.shape))\n",
        "print('wavelengths number = {}'.format(np.shape(x_scale)))"
      ],
      "metadata": {
        "id": "JJjqCePhZbHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Visual data sanity checking for the test set\n",
        "plt.figure(figsize=(16,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(x_test[:50,:].T,'k')\n",
        "plt.title('First 50 spectra from the tuning sample')\n",
        "plt.xlabel(r'$\\lambda$ (nm)')\n",
        "plt.ylabel('Spectra intensity')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.title('Y labels')\n",
        "plt.plot(y_test,'k.')\n",
        "plt.xlabel('Sample number')\n",
        "plt.ylabel('Dry Matter (%)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "55QK1BXjFLMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Since the test set is unknown (we are not suppose to have access to it during the\n",
        "## optimization of the model) the scalling process should take this into account. We\n",
        "## have to define a scaler based only on the train data, and apply it to the test data.\n",
        "\n",
        "def standardize_column(X_train, X_val, x_test):\n",
        "    ## We train the scaler on the full train set and apply it to the other datasets\n",
        "    scaler = **missing data**\n",
        "    ## for columns we fit the scaler to the train set and apply it to the test set\n",
        "    X_train_scaled = scaler.transform(X_train)\n",
        "    X_val_scaled = scaler.transform(X_val)\n",
        "    X_test_scaled = scaler.transform(x_test)\n",
        "    return [X_train_scaled, X_val_scaled, X_test_scaled]"
      ],
      "metadata": {
        "id": "MVlFTiadFOU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Standardize on columns\n",
        "X_train_scaled, X_val_scaled, X_test_scaled= **missing data**"
      ],
      "metadata": {
        "id": "7WQ1GcaeFR0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(18,4))\n",
        "## left panel\n",
        "plt.subplot(121)\n",
        "plt.title('A', fontsize=18)\n",
        "plt.plot(x_test[:50].T)\n",
        "plt.ylabel('Unscaled amplitude')\n",
        "plt.xlabel('Feature number')\n",
        "## vertical dashed lines separating the types of preprocessings that were concatenated\n",
        "plt.vlines(np.arange(103,600,103), ymin=-1.1, ymax=5.1, color='k',ls='--',lw=1)\n",
        "plt.xlim(-5,625)\n",
        "plt.ylim(-1,5)\n",
        "\n",
        "## right panel\n",
        "plt.subplot(122)\n",
        "plt.title('B', fontsize=18)\n",
        "plt.plot(X_test_scaled[:50].T)\n",
        "plt.ylabel('Standardized amplitude')\n",
        "plt.xlabel('Feature number')\n",
        "plt.vlines(np.arange(103,600,103), ymin=-7.6, ymax=5.6, color='k',ls='--',lw=1)\n",
        "plt.xlim(-5,625)\n",
        "plt.ylim(-7.5,5.5)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ed5wa4DyFZar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "**missing package**"
      ],
      "metadata": {
        "id": "0hE8B8RG5LQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from lazypredict.Supervised import LazyRegressor\n",
        "\n",
        "reg = LazyRegressor(verbose=0, ignore_warnings=False, custom_metric=None)\n",
        "models, predictions = reg.fit(X_train_scaled, X_test_scaled, Y_train, y_test)\n",
        "\n",
        "print(models)"
      ],
      "metadata": {
        "id": "B3GnUi1X5I_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv1D, Reshape, GaussianNoise\n",
        "from keras.callbacks import ReduceLROnPlateau"
      ],
      "metadata": {
        "id": "47Mq41d-FcVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_model(filter_size, reg_beta):\n",
        "**missing data**\n",
        "\n",
        "\n",
        "    return model_cnn"
      ],
      "metadata": {
        "id": "qLN59ac4Il-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Function to compute metrics and make prediction plots\n",
        "def plot_prediction(X_calib, Y_calib, X_valid, Y_valid, X_test, Y_test, Y_calib_pred, Y_valid_pred, Y_test_pred, savefig=False, figname=None):\n",
        "\n",
        "    ## Compute train error scores\n",
        "    score_p0 = r2_score(Y_calib, Y_calib_pred)\n",
        "    mse_p0 = mean_squared_error(Y_calib, Y_calib_pred)\n",
        "    rmse_p0 = np.sqrt(mse_p0)\n",
        "\n",
        "    ## Compute valid error scores\n",
        "    score_p1 = r2_score(Y_valid, Y_valid_pred)\n",
        "    mse_p1 = mean_squared_error(Y_valid, Y_valid_pred)\n",
        "    rmse_p1 = np.sqrt(mse_p1)\n",
        "\n",
        "    ## Compute test error scores\n",
        "    score_p2 = r2_score(Y_test, Y_test_pred)\n",
        "    mse_p2 = mean_squared_error(Y_test, Y_test_pred)\n",
        "    rmse_p2 = np.sqrt(mse_p2)\n",
        "\n",
        "\n",
        "    print('ERROR METRICS: \\t TRAIN \\t\\t VALID \\t\\t TEST')\n",
        "    print('------------------------------------------------------')\n",
        "    print('R2: \\t\\t %5.3f \\t\\t %5.3f \\t\\t %5.3f'  % (score_p0, score_p1, score_p2 ))\n",
        "    print('RMSE: \\t\\t %5.3f \\t\\t %5.3f \\t\\t %5.3f' % (rmse_p0, rmse_p1,  rmse_p2))\n",
        "\n",
        "\n",
        "    ## Plot regression for PLS predicted data\n",
        "    rangey = max(Y_test) - min(Y_test)\n",
        "    rangex = max(Y_test_pred) - min(Y_test_pred)\n",
        "\n",
        "    fig=plt.figure(figsize=(6,6))\n",
        "    z = np.polyfit(np.ravel(Y_test), np.ravel(Y_test_pred), 1)\n",
        "    ax = plt.subplot(aspect=1)\n",
        "    ax.scatter(Y_test,Y_test_pred,c='k',marker='o',s=20, alpha=0.6)\n",
        "    ax.plot(Y_test, z[1]+z[0]*Y_test, c='blue', linewidth=2,label='linear fit')\n",
        "    ax.plot(Y_test, Y_test, 'k--', linewidth=1.5, label='y=x')\n",
        "    plt.ylabel('Predicted DM')\n",
        "    plt.xlabel('Measured DM')\n",
        "    plt.title('Prediction from CNN')\n",
        "    plt.legend(loc=4)\n",
        "\n",
        "\n",
        "    # Print the scores on the plot\n",
        "    plt.text(min(Y_test_pred)+0.02*rangex, max(Y_test)-0.1*rangey, 'R$^{2}=$ %5.3f'  % score_p2)\n",
        "    plt.text(min(Y_test_pred)+0.02*rangex, max(Y_test)-0.15*rangey, 'RMSE: %5.3f' % rmse_p2)\n",
        "    if savefig==True:\n",
        "        plt.savefig(figname, dpi=96)\n",
        "        print('Figure saved')\n",
        "    else:\n",
        "        plt.show()\n",
        "    return"
      ],
      "metadata": {
        "id": "FutV_SiGhSpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Initialize a new CNN with a filter size of 5 points and L2 reg. = 0.0006 (weak regularization)\n",
        "model_cnn = init_model(5, 0.0006)\n",
        "\n",
        "## Print the summary of the model\n",
        "print(model_cnn.summary())"
      ],
      "metadata": {
        "id": "pBSaesxyV2n2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Plot the architecture of the CNN\n",
        "## The plot_model() from Keras requires that graphviz and pydot are installed in your system.\n",
        "## If you dont't have these libraries installed, skip this cell for now and browse for the image \"base_cnn_regression.png\" in the repository to see the result\n",
        "plot_model(**missingdata**)"
      ],
      "metadata": {
        "id": "_leXY6VUjzYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########### DEFINE HYPERPARAMETERS AND INSTANTIATE THE MODEL #####################\n",
        "## Define the batch size (number of sample to train for each pass into the CNN)\n",
        "BATCH = 256\n",
        "## Define the filter size\n",
        "FILTER_SIZE = 5\n",
        "## Define the L2 reg.\n",
        "L2_BETA = 0.0006\n",
        "\n",
        "## initialize the model\n",
        "model_cnn=init_model(FILTER_SIZE, L2_BETA)"
      ],
      "metadata": {
        "id": "EvaDCTjnIpva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Run this base model once to see if its training correctly\n",
        "## This can be viewed as a baseline that hyperparameter optimization should improve\n",
        "\n",
        "tf.keras.backend.clear_session()"
      ],
      "metadata": {
        "id": "rWcnMWDGWN0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########### COMPILE MODEL WITH ADAM OPTIMIZER #####################################\n",
        "\n",
        "## Heuristic that sets the learning rate\n",
        "LR=** missing data**\n",
        "print('Adam learning rate = {}'.format(LR))\n",
        "\n",
        "## Define the model name for saving purposes\n",
        "MODEL_NAME = 'base_regression_model.h5'\n",
        "\n",
        "## Compile the model defining the optimizer, the loss function and the metrics to track during training\n",
        "model_cnn.compile(**missing data**)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsUATJtZWBDU",
        "outputId": "a291ec0b-09a9-4f33-80c4-20383d3d2a2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adam learning rate = 0.01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########### DEFINE USEFUL CALLBACK FUNCTIONS #####################################\n",
        "\n",
        "## The following are several callbacks that can be used during training to improve its efficiency and decrease overfit\n",
        "## 1) Stop the training if it does not improve\n",
        "early_stop = EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=50, mode='auto', restore_best_weights=True)\n",
        "\n",
        "## 2) Reduce learning rate dynamically\n",
        "rdlr = ReduceLROnPlateau(patience=25, factor=0.5, min_lr=1e-6, monitor='val_loss', verbose=0)\n",
        "\n",
        "## 3) Save the best weights into file\n",
        "checkpointer = ModelCheckpoint(filepath=MODEL_NAME, verbose=1, save_best_only=True)\n"
      ],
      "metadata": {
        "id": "etIDvyqpWDO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########### TRAIN THE MODEL #####################################################\n",
        "## Train the model for 450 epochs on the x_cal data while monitoring the models performance on the x_tuning data.\n",
        "## The data in the tuning set is not used for actual training. It is only used to measure model performance.\n",
        "h1 = model_cnn.fit(**missing data**)\n",
        "\n",
        "## Clear the session. This is important to ensure that on the next training session, the weights are properly initialized.\n",
        "## On the other hand if one need to continue a training session for longer, this can just be commented.\n",
        "tf.keras.backend.clear_session()"
      ],
      "metadata": {
        "id": "7G_Rdov0WGaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## If you used Livelossplot, you can skip this. Otherwise we can take a look at the training process by plotting the\n",
        "## models history.\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.plot(h1.history['loss'], label='Calibration set loss')\n",
        "plt.plot(h1.history['val_loss'], label='Tunning set loss')\n",
        "plt.yscale('log')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epochs')\n",
        "# plt.ylim(0.5,1)\n",
        "plt.legend()\n",
        "## In case you used ReduceLROnPlateau() you can plot the lr as well\n",
        "ax2 = plt.gca().twinx()\n",
        "ax2.plot(h1.history['lr'], color='r', ls='--')\n",
        "ax2.set_ylabel('learning rate',color='r')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "# plt.savefig('train_tun_loss.png', dpi=96)"
      ],
      "metadata": {
        "id": "3-Pfo4C-JQOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Initialize a model with the best found hyperparameters\n",
        "model_cnn = **missing data**\n",
        "\n",
        "## Load the weights of the pre-trained / saved corresponding model\n",
        "## load pre-computed model weights into model_cnn\n",
        "model_cnn.load_weights(\"/content/base_regression_model.h5\")\n",
        "\n",
        "## Compute metrics for Training, validation and Test sets\n",
        "y_train_pred = model_cnn.predict(X_train_scaled)\n",
        "y_val_pred3 = model_cnn.predict(X_val_scaled)\n",
        "y_test_pred3  = model_cnn.predict(X_test_scaled)\n",
        "\n"
      ],
      "metadata": {
        "id": "HvIGBD_uRpcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Plot the prediction metrics using the custom function \"plot_prediction\", defined in the initial Help section.\n",
        "plot_prediction(X_train_scaled,Y_train, X_val_scaled, Y_val, X_test_scaled, y_test, y_train_pred, y_val_pred3, y_test_pred3, savefig=False, figname='prediction_base_model1.png')\n"
      ],
      "metadata": {
        "id": "S1ozpzvgfheM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}